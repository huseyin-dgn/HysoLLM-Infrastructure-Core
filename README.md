
# ğŸ§  HysoLLM â€“ Makine Ã–ÄŸrenimi & LLM EÄŸitim AltyapÄ±sÄ±

HysoLLM, makine Ã¶ÄŸrenimi ve Ã¶zellikle bÃ¼yÃ¼k dil modeli (LLM) projelerinde **eÄŸitim sÃ¼reÃ§lerini dÃ¼zenlemek, deneyleri kaydetmek, yeniden Ã¼retilebilir hale getirmek ve modÃ¼ler bir mimari Ã¼zerine oturtmak** amacÄ±yla geliÅŸtirilmiÅŸ hafif bir frameworkâ€™tÃ¼r.

Bu altyapÄ±, modern ML projelerinde ihtiyaÃ§ duyulan temel bileÅŸenleri tek Ã§atÄ± altÄ±nda toplar:

- âš™ï¸ ModÃ¼ler model mimarileri  
- ğŸ”¤ Tokenizer sistemi (BPE + Simple)  
- ğŸ“¦ Deney yÃ¶netimi (run paths, manifest, logger, checkpoints, seed)  
- âš¡ EÄŸitim dÃ¶ngÃ¼sÃ¼ (trainer, callback sistemi)  
- ğŸ§¾ Config yÃ¶netimi (YAML/JSON + CLI override)  
- ğŸ“ Harici config dosyalarÄ± (configs/)  

---
## **ğŸš€ KullanÄ±m Ã–rneÄŸi**

- AÅŸaÄŸÄ±da HysoLLM bileÅŸenlerinin birlikte nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶steren gerÃ§ek bir Ã¶rnek bulunmaktadÄ±r.
AmaÃ§:  
**Model kodunu eÄŸitim mekanizmalarÄ±ndan tamamen ayÄ±rarak**, farklÄ± projelerde yeniden kullanÄ±labilir, dÃ¼zenli ve sÃ¼rdÃ¼rÃ¼lebilir bir yapÄ± oluÅŸturmak.
```python
import sys
sys.path.append(r"C:\Users\hdgn5\OneDrive\MasaÃ¼stÃ¼\hysollm")

import json
import torch

# ======================================
# Hyso Core (config + storage)
# ======================================
from hyso.core.config import load_config_with_overrides
from hyso.core.storage import (
    RunPathFactory,
    get_logger,
    Manifest,
    save_manifest,
    set_global_seed,
    CheckpointConfig,
    CheckpointManager,
)

# ======================================
# Hyso Train & Tokenizer
# ======================================
from hyso.core.train.trainer import HysoTrainer
from hyso.core.train.metrics import LLMetrics
from hyso.core.train.callbacks import HysoCallbacks
from hyso.core.train.dataloader import build_dataloader   # <-- KÃœTÃœPHANEDE VAR
from hyso.core.tokenizer.bpe_tokenizer import HysoBPETokenizer

# ======================================
# Hyso Model
# ======================================
import hyso

# =====================================================================
# 1) CONFIG YÃœKLE (YAML dosyasÄ±ndan + CLI override desteÄŸi)
# =====================================================================

cfg = load_config_with_overrides(
    path="configs/base.yaml",
    override_pairs=sys.argv[1:],   # Ã–rn: training.lr=0.0001
)

# =====================================================================
# 2) STORAGE YAPISI KUR (RunPaths, Logger, Seed, Manifest)
# =====================================================================

factory = RunPathFactory.from_root("runs")
run_paths = factory.create()

logger = get_logger("train", log_dir=run_paths.logs_dir)
logger.info(f"Yeni run baÅŸlatÄ±ldÄ±: {run_paths.run_id}")

set_global_seed(cfg.training.seed)

# =====================================================================
# 3) VERÄ°SETÄ°NÄ°N NEREDE DURMASI GEREKÄ°R?
# =====================================================================
"""
GerÃ§ek ve profesyonel dizin yapÄ±sÄ±:

project/
   data/
      train.json
      val.json
      test.json

Dosya formatÄ±:
[
  {"src": "merhaba dÃ¼nya", "tgt": "hello world"},
  {"src": "nasÄ±lsÄ±n", "tgt": "how are you"},
  ...
]

Bu format seq2seq iÃ§in *standarttÄ±r* ve HysoTrainer'Ä±n collate yapÄ±sÄ±yla
doÄŸrudan uyumludur.
"""

with open("data/train.json", "r", encoding="utf-8") as f:
    train_records = json.load(f)

with open("data/val.json", "r", encoding="utf-8") as f:
    val_records = json.load(f)

logger.info(f"Verisetleri yÃ¼klendi. Train: {len(train_records)}, Val: {len(val_records)}")

# =====================================================================
# 4) TOKENIZER TRAIN 
# =====================================================================

corpus = [item["src"] for item in train_records] + \
         [item["tgt"] for item in train_records]

tok = HysoBPETokenizer(
    lowercase=False,
    normalize="NFKC",
    cache_size=10000,
)

tok.fit(
    corpus=corpus,
    target_vocab_size=cfg.tokenizer.vocab_size,
    min_pair_freq=2,
)

logger.info(f"Tokenizer hazÄ±r. Vocab size: {tok.vocab_size}")

# =====================================================================
# 5) DATALOADER â€” KÃœTÃœPHANEDE KENDÄ° BUILD_DATALOADER VAR
# =====================================================================

train_loader = build_dataloader(
    dataset=train_records,     # list[dict{src,tgt}]
    mode="seq2seq",
    tokenizer=tok,
    batch_size=cfg.training.batch_size,
    max_src_len=cfg.data.max_src_len,
    max_tgt_len=cfg.data.max_tgt_len,
    shuffle=True,
)

val_loader = build_dataloader(
    dataset=val_records,
    mode="seq2seq",
    tokenizer=tok,
    batch_size=cfg.training.batch_size,
    max_src_len=cfg.data.max_src_len,
    max_tgt_len=cfg.data.max_tgt_len,
    shuffle=False,
)

# =====================================================================
# 6) MODEL OLUÅTUR
# =====================================================================

model = hyso.HysoLLM(
    vocab_src=tok.vocab_size,
    vocab_tgt=tok.vocab_size,
    d_model=cfg.model.dim,
    n_heads=cfg.model.heads,
).to(cfg.training.device)

logger.info("Model oluÅŸturuldu.")

# =====================================================================
# 7) METRICS + CALLBACKS + CHECKPOINTMANAGER
# =====================================================================

metrics = LLMetrics(
    bleu=False,
    perplexity=True,
    token_accuracy=True,
    token_ignore_index=-100,
)

callbacks = HysoCallbacks.default(
    total_steps=len(train_loader) * cfg.training.epochs,
    save_dir=str(run_paths.checkpoints_dir),
)

ckpt_cfg = CheckpointConfig.from_dir(
    directory=run_paths.checkpoints_dir,
    best_metric_name="val_loss",
    best_mode="min",
)

checkpoint_manager = CheckpointManager(ckpt_cfg)

# =====================================================================
# 8) TRAINER â€” KÃœTÃœPHANE TAM ENTEGRE
# =====================================================================

trainer = HysoTrainer(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    tokenizer=tok,
    epochs=cfg.training.epochs,
    lr=cfg.training.lr,
    optimizer=cfg.training.optimizer,
    scheduler=cfg.training.scheduler,
    warmup_ratio=cfg.training.warmup_ratio,
    ignore_index=-100,
    callbacks=callbacks,
    metrics=metrics,
    logger=logger,
    run_paths=run_paths,
    checkpoint_manager=checkpoint_manager,
    config=cfg,
)

# =====================================================================
# 9) EÄÄ°TÄ°M BAÅLAT
# =====================================================================

result = trainer.fit()
logger.info("EÄŸitim tamamlandÄ±.")


```
---

# ğŸ“ Proje YapÄ±sÄ±
Her klasÃ¶r aÅŸaÄŸÄ±da aÃ§Ä±klanmaktadÄ±r.
```bash
core/
  models/
  tokenizer/
  storage/
  train/
  config/
  configs/
```

---

# ğŸ”¤ 1. Tokenizer ModÃ¼lÃ¼ (`hyso/core/tokenizer/`)

Tokenizer yapÄ±sÄ± metni modele uygun ID dizilerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. HysoLLM iki tokenizer iÃ§erir:

---

### **â€¢ HysoBPETokenizer (BPE)**  
Byte Pair Encoding tabanlÄ± tokenizer.

**YÃ¶ntemler & yetenekler:**
- BPE merge kurallarÄ±na dayalÄ± alt birim Ã¼retimi  
- Ã–zel token ID yÃ¶netimi  
- Batch encode/decode  
- Seq2Seq encoder ve decoder iÃ§in ayrÄ± encode fonksiyonlarÄ±  
- UTF-8 byte-level iÅŸleme desteÄŸi  

**KullanÄ±lan Teknolojiler:**
- Python  
- PyTorch (tensor dÃ¶nÃ¼ÅŸÃ¼mleri)  
- Unicode processing  
- BPE algoritmasÄ±  

---

### **â€¢ HysoSimpleTokenizer (Word-level)**  
Kelime temelli sade tokenizer.

**YÃ¶ntemler:**
- Temel whitespace tokenizasyonu  
- Kelime frekansÄ±na gÃ¶re vocabulary oluÅŸturma  
- Batch encode/decode  
- Mask Ã¼retimi  

**KullanÄ±lan Teknolojiler:**
- Python  
- PyTorch  
- Temel NLP tokenizasyon teknikleri  

---

# ğŸ§© 2. Model ModÃ¼lÃ¼ (`hyso/core/models/`)

Bu klasÃ¶r HysoLLM model mimarilerini iÃ§erir. TÃ¼m modeller modÃ¼ler ve geniÅŸletilebilir yapÄ±dadÄ±r.

---

### **Model mimarileri:**
- **Encoder-Decoder Transformer**  
- **Encoder-only LLM** (BERT/LLM tarzÄ±)  
- **Decoder-only Model** (GPT tarzÄ±)

---

### **Her modelin sunduÄŸu yÃ¶ntemler:**
- `forward()`  
- `generate()` (decoder-only)  
- `encode()` / `decode()` (enc-dec yapÄ±larÄ±)  
- RoPE pozisyonel embedding  
- Attention katmanlarÄ±  
- DropPath, LayerNorm, RMSNorm gibi modern bileÅŸenler  

**KullanÄ±lan Teknolojiler:**
- PyTorch (nn.Module)  
- Multi-Head Attention  
- Rotary Positional Embedding (RoPE)  
- SwiGLU / GeLU aktivasyonlarÄ±  
- Residual / PreNorm mimarileri  

---

# ğŸ“¦ 3. Storage ModÃ¼lÃ¼ (`hyso/core/storage/`)

Bu modÃ¼l HysoLLMâ€™in gerÃ§ek â€œdeney yÃ¶netimiâ€ Ã§ekirdeÄŸidir.
KlasÃ¶r yapÄ±sÄ±nÄ± otomatik oluÅŸturur.

**YÃ¶ntemler:**  
- `create()`  
- `run_id` Ã¼retimi  
- klasÃ¶r kurulumu  

## **â€¢ Logger**  
Hem konsola hem dosyaya temiz log yazan yapÄ±landÄ±rÄ±labilir logger.

**YÃ¶ntemler:**  
- `get_logger(name, log_dir)`  
- Formatlama (timestamp, level, name, message)  
- Rotating log desteÄŸi  


## **â€¢ Manifest**  
Her eÄŸitimin kimlik kartÄ±dÄ±r.

Ä°Ã§erik:  
- model bilgisi  
- eÄŸitim bilgisi  
- veri seti bilgisi  
- ortam (Python, OS, CUDA)  
- hyperparametreler  
- timestamp  

**YÃ¶ntemler:**  
- `Manifest.new()`  
- `save_manifest()`  
- `load_manifest()`  

## **â€¢ CheckpointManager**  
EÄŸitim sÄ±rasÄ±nda modelleri gÃ¼venli ÅŸekilde kaydeder.

**YÃ¶ntemler:**
- `save(epoch, model, optimizer, scheduler)`  
- `load_latest()`  
- `load_best()`  
- `restore_objects()`  
- max_to_keep ile eski checkpoint silme  


## **â€¢ Seed YÃ¶netimi**
TÃ¼m ortamÄ± deterministik hale getirir.

**YÃ¶ntemler:**  
- `set_global_seed(seed)`  

**KullanÄ±lan Teknolojiler (tÃ¼m storage):**
- PyTorch  
- Python logging  
- JSON  
- UUID  
- Pathlib  
- Datetime  

---

# âš¡ 4. Train ModÃ¼lÃ¼ (`hyso/core/train/`)

Bu modÃ¼l model eÄŸitimi iÃ§in gerekli tÃ¼m yapÄ±lara sahiptir.

### **Ä°Ã§erik:**

### **â€¢ Trainer**
Model, veri, optimizer, scheduler, storage ve config bileÅŸenlerini bir araya getiren ana eÄŸitim sÄ±nÄ±fÄ±.

**YÃ¶ntemler:**
- `train_epoch()`  
- `validate()`  
- `fit()`  
- Loss hesaplama  
- Metrics kaydetme  


### **â€¢ Callback Sistemi**
EÄŸitimin belirli aÅŸamalarÄ±nda Ã§alÄ±ÅŸacak kÃ¼Ã§Ã¼k kancalar.

**YÃ¶ntemler:**
- `on_train_start()`  
- `on_epoch_end()`  
- `on_step_end()`  
- CallbackList ile Ã§oklu callback desteÄŸi  

### **â€¢ Metrics Logging**
EÄŸitim ve validasyon metriklerini CSVâ€™e yazar.

**KullanÄ±lan Teknolojiler:**
- PyTorch (dataset, dataloader, optim, scheduler)  
- Callback pattern  
- CSV logging  
- Checkpoint entegrasyonu  

# ğŸ§¾ 5. Config ModÃ¼lÃ¼ (`hyso/core/config/`)

Config sistemi ayarlarÄ± koddan ayÄ±rÄ±r ve tÃ¼m eÄŸitim sÃ¼reÃ§lerini yapÄ±landÄ±rÄ±labilir hale getirir.


### **YapÄ±lan iÅŸlemler:**
- YAML / JSON config dosyasÄ± yÃ¼kleme  
- CLI override desteÄŸi  
  (`training.lr=0.0001 model.layers=12` gibi)  
- Deep merge  
- Config kaydetme / okuma  
- Attribute-style eriÅŸim:  
  `cfg.training.lr`  


### **YÃ¶ntemler:**
- `load_config(path)`  
- `parse_overrides(argv)`  
- `merge_config(base, override)`  
- `load_config_with_overrides()`  
- `save_config()`  

**KullanÄ±lan Teknolojiler:**
- PyYAML  
- JSON  
- Python AST parsing  
- Recursive merge algoritmasÄ±  

---

# ğŸ“ 6. Configs KlasÃ¶rÃ¼ (`configs/`)

Bu klasÃ¶r train modÃ¼lÃ¼ iÃ§in harici ayar dosyalarÄ±nÄ± iÃ§erir.

Ã–rnek dosyalar:
- `base.yaml`
- `encoder_small.yaml`
- `encoder_large.yaml`
- `lr_sweep.yaml`

Bu sayede:

- Deney ayarlarÄ± versiyonlanabilir,  
- FarklÄ± modeller iÃ§in hÄ±zlÄ± switching yapÄ±labilir,  
- Manifest + config birleÅŸince tam reproducibility saÄŸlanÄ±r.

**KullanÄ±lan Teknolojiler:**  
- YAML  
- JSON  

---

# ğŸ§© Ã–zet

HysoLLM yalnÄ±zca model mimarisi deÄŸil;  
**tam bir eÄŸitim altyapÄ±sÄ±, deney yÃ¶netim sistemi, config framework ve modÃ¼ler ML yapÄ± setidir.**

Bu repo:

- ğŸ”¤ Tokenizer sistemini  
- âš™ï¸ Model mimarisini  
- ğŸ“¦ Storage ve deney yÃ¶netimini  
- âš¡ EÄŸitim altyapÄ±sÄ±nÄ±  
- ğŸ§¾ Config yÃ¶netimini  

birbirinden ayrÄ±lmÄ±ÅŸ, temiz ve profesyonel ÅŸekilde organize eder.

--- 

# âœ¨ Lisans
MIT License



